{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Hyperparameter tuning\n",
    "import optuna\n",
    "\n",
    "# Models\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# Data processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data input\n",
    "data = pd.read_csv('../data/train.csv')\n",
    "data_to_predict = pd.read_csv('../data/test.csv')\n",
    "data_ccrisk = pd.read_csv('../data/credit_risk_dataset.csv')\n",
    "\n",
    "X_old = data.drop(['loan_status'], axis=1)\n",
    "y_old = data['loan_status']\n",
    "\n",
    "# Merge the dataframes\n",
    "data_no_id = data.drop(['id'], axis=1)\n",
    "merged_data = pd.concat([data_no_id, data_ccrisk], ignore_index=True)\n",
    "\n",
    "X = merged_data.drop(['loan_status'], axis=1)\n",
    "y = merged_data['loan_status']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, shuffle=True, test_size=0.2, random_state=2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing\n",
    "\n",
    "categorical_columns = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "categorical_ordinal = ['loan_grade']\n",
    "categorical_onehot = categorical_columns.drop(categorical_ordinal)\n",
    "\n",
    "numerical_columns = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "\ttransformers=[\n",
    "\t\t('ordinal', OrdinalEncoder(), categorical_ordinal),\n",
    "\t\t('onehot', OneHotEncoder(), categorical_onehot),\n",
    "\t\t('scaler', StandardScaler(), numerical_columns)\n",
    "\t])\n",
    "\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "X_train_prep = preprocessor.transform(X_train)\n",
    "X_test_prep = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-09 21:08:31,030] A new study created in memory with name: no-name-ef6fb6ae-f37b-4b49-abb2-ddc88c5b12f3\n",
      "[I 2024-10-09 21:08:33,361] Trial 0 finished with value: 0.939108817007827 and parameters: {'max_depth': 3, 'learning_rate': 0.07465517037951955, 'n_estimators': 135, 'subsample': 0.7369438291641859, 'colsample_bytree': 0.8923076783150959, 'gamma': 0.30157194902631745, 'lambda': 1.6268250332604492, 'alpha': 1.2805043154157706e-05, 'scale_pos_weight': 2.6463118946663324}. Best is trial 0 with value: 0.939108817007827.\n",
      "[I 2024-10-09 21:08:37,561] Trial 1 finished with value: 0.9550097865450714 and parameters: {'max_depth': 4, 'learning_rate': 0.09141033241296674, 'n_estimators': 226, 'subsample': 0.7663718146248113, 'colsample_bytree': 0.6098644847778827, 'gamma': 0.23092627109255753, 'lambda': 0.31921581391424503, 'alpha': 6.129147756852991, 'scale_pos_weight': 2.191348000078599}. Best is trial 1 with value: 0.9550097865450714.\n",
      "[I 2024-10-09 21:08:43,280] Trial 2 finished with value: 0.9547313706916878 and parameters: {'max_depth': 10, 'learning_rate': 0.22998641091530314, 'n_estimators': 224, 'subsample': 0.835933506378937, 'colsample_bytree': 0.9485449802089733, 'gamma': 0.45313543345915913, 'lambda': 1.5727902356093326, 'alpha': 2.759278689901208, 'scale_pos_weight': 1.1930241911516504}. Best is trial 1 with value: 0.9550097865450714.\n",
      "[I 2024-10-09 21:08:50,954] Trial 3 finished with value: 0.9444244458915206 and parameters: {'max_depth': 4, 'learning_rate': 0.01919437737456651, 'n_estimators': 443, 'subsample': 0.8586267222525115, 'colsample_bytree': 0.5777479344178063, 'gamma': 0.18089959547337325, 'lambda': 6.718458063669943e-08, 'alpha': 1.1524647932242826e-07, 'scale_pos_weight': 2.7256946622714464}. Best is trial 1 with value: 0.9550097865450714.\n",
      "[I 2024-10-09 21:09:00,576] Trial 4 finished with value: 0.9592126910591579 and parameters: {'max_depth': 6, 'learning_rate': 0.03707733325217397, 'n_estimators': 499, 'subsample': 0.850224682179192, 'colsample_bytree': 0.7844597004626956, 'gamma': 0.03183686314336448, 'lambda': 0.0005327343359185902, 'alpha': 0.00038750873676980744, 'scale_pos_weight': 2.0423308194709486}. Best is trial 4 with value: 0.9592126910591579.\n",
      "[I 2024-10-09 21:09:05,545] Trial 5 finished with value: 0.9569448466208333 and parameters: {'max_depth': 3, 'learning_rate': 0.18868813249647445, 'n_estimators': 314, 'subsample': 0.9363621123141573, 'colsample_bytree': 0.7577668067649601, 'gamma': 0.38920616856158546, 'lambda': 4.31263606241349e-06, 'alpha': 0.0024350742991795563, 'scale_pos_weight': 2.6521428924189543}. Best is trial 4 with value: 0.9592126910591579.\n",
      "[I 2024-10-09 21:09:10,422] Trial 6 finished with value: 0.9570769743697349 and parameters: {'max_depth': 9, 'learning_rate': 0.0944392175532155, 'n_estimators': 190, 'subsample': 0.9000631509130953, 'colsample_bytree': 0.8918033408930562, 'gamma': 0.17928798683043334, 'lambda': 6.717384652294103e-05, 'alpha': 1.2243486689358441e-08, 'scale_pos_weight': 2.2166025166627166}. Best is trial 4 with value: 0.9592126910591579.\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have X_train_prep, y_train, X_test_prep, y_test ready\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Define the hyperparameter search space\n",
    "    param = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0.0, 0.5),\n",
    "        'lambda': trial.suggest_float('lambda', 1e-8, 10.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-8, 10.0, log=True),\n",
    "        'scale_pos_weight': trial.suggest_float('scale_pos_weight', 1.0, 3.0),\n",
    "        'eval_metric': 'auc',  # Ensure AUC is the metric being optimized\n",
    "        'objective': 'binary:logistic'  # Use binary logistic since you want probability outputs\n",
    "    }\n",
    "\n",
    "    # Initialize the model with current hyperparameters\n",
    "    model = xgb.XGBClassifier(**param)\n",
    "\n",
    "    # Fit the model (using early stopping)\n",
    "    model.fit(X_train_prep, y_train, \n",
    "              eval_set=[(X_test_prep, y_test)],  \n",
    "              verbose=False)\n",
    "\n",
    "    # Get the predicted probabilities for the validation set\n",
    "    y_pred_prob = model.predict_proba(X_test_prep)[:, 1]\n",
    "\n",
    "    # Calculate the AUC score\n",
    "    auc = roc_auc_score(y_test, y_pred_prob)\n",
    "    \n",
    "    return auc\n",
    "\n",
    "# Create a study object and maximize the objective\n",
    "study = optuna.create_study(direction='maximize')  # Because we want to maximize AUC\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Output the best trial\n",
    "print(f\"Best trial: {study.best_trial.params}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
